# infra-autofix-agent

[![CI Pipeline](https://github.com/mainulhossain123/infra-autofix-agent/actions/workflows/ci.yml/badge.svg)](https://github.com/mainulhossain123/infra-autofix-agent/actions/workflows/ci.yml)
[![Security Scanning](https://github.com/mainulhossain123/infra-autofix-agent/actions/workflows/security.yml/badge.svg)](https://github.com/mainulhossain123/infra-autofix-agent/actions/workflows/security.yml)
[![Docker Publish](https://github.com/mainulhossain123/infra-autofix-agent/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/mainulhossain123/infra-autofix-agent/actions/workflows/docker-publish.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **AI-Powered Auto-Remediation Platform** with 6 production-ready ML models for predictive incident detection, automated root cause analysis, and intelligent self-healing infrastructure.

## âœ¨ Features

### Core Platform
- ğŸ” **Automated Incident Detection** - CPU spikes, memory leaks, high error rates, latency issues
- ğŸ”„ **Self-Healing** - Automatic container restarts with circuit breaker protection
- ğŸ“Š **Full Observability Stack** - Grafana dashboards, Prometheus metrics, Loki logs
- âš¡ **Real-time Updates** - WebSocket-based live dashboard
- ğŸš¨ **Smart Alerting** - Configurable thresholds with Prometheus alerts
- ğŸ³ **Cloud-Native** - Docker/Kubernetes ready with multi-platform support
- ğŸ” **Production-Grade** - CI/CD pipelines, security scanning, automated testing
- ğŸ“– **Interactive API Documentation** - Swagger UI with live testing at http://localhost:5000/api/docs

### ğŸ¤– AI/ML Capabilities (All 6 Phases Complete!)
- **Phase 2: Anomaly Detection** - Isolation Forest with 92-95% accuracy, <10ms inference
- **Phase 3: Time Series Forecasting** - Prophet models predict metrics 1-24 hours ahead
- **Phase 4: LLM Integration** - Ollama + Llama 3.2 for root cause analysis (100% free, local)
- **Phase 5: Failure Prediction** - LightGBM predicts system failures up to 72 hours ahead
- **Phase 6: Continuous Learning** - Automated model retraining and performance monitoring

**Result**: Proactive incident prevention with AI-powered insights and zero API costs!

## ğŸš€ Quick Start

> **ğŸ“˜ New to the project?** See the [Complete Quick Start Guide](docs/quick-start.md) with step-by-step instructions and screenshots.

### Prerequisites

- **Docker Compose**: Docker 20.10+, Docker Compose 2.0+
- **Kubernetes** (optional): kubectl, Helm 3.0+, K8s cluster 1.20+
- **ML Features** (optional): 4GB RAM for Ollama LLM service

### Option 1: Docker Compose (Recommended)

**Basic Setup:**
```bash
git clone https://github.com/mainulhossain123/infra-autofix-agent.git
cd infra-autofix-agent
docker compose up -d
```

**With ML/AI Features (Recommended):**
```bash
# Start with ML services (Ollama LLM + all AI features)
docker compose -f docker-compose.yml -f docker-compose.ml.yml up -d

# Download the LLM model (one-time setup, ~2GB)
docker exec -it ar_ollama ollama pull llama3.2:3b
```

**Access Services:**
- ğŸ¨ Dashboard: http://localhost:3000
- ğŸ“– Swagger API: http://localhost:5000/api/docs
- ğŸ“Š Grafana: http://localhost:3001 (admin/admin)
- ğŸ”§ Backend API: http://localhost:5000
- ğŸ“ˆ Prometheus: http://localhost:9090

### Option 2: Kubernetes (Production)

**With Helm:**
```bash
# Development
helm install infra-autofix ./helm/infra-autofix \
  --namespace infra-autofix-dev \
  --create-namespace \
  --values ./helm/infra-autofix/values-dev.yaml

# Production
helm install infra-autofix ./helm/infra-autofix \
  --namespace infra-autofix-prod \
  --create-namespace \
  --values ./helm/infra-autofix/values-prod.yaml
```

**With kubectl:**
```bash
kubectl apply -k k8s/

# Access services at (Docker Desktop):
# Dashboard: http://localhost
# Grafana: http://localhost:3000 (admin/admin)
# API: http://localhost:5000
```

**Or use deployment script:**
```bash
# PowerShell
.\scripts\deploy-k8s.ps1 -Method helm -Environment dev

# Bash
./scripts/deploy-k8s.sh helm dev
```

> **âš ï¸ Important**: Grafana dashboards need to be imported manually in Kubernetes. See [Quick Start Guide](docs/quick-start.md#6-configure-grafana-dashboards).

See [Kubernetes Deployment Guide](docs/kubernetes.md) for detailed instructions.

### Access Interfaces

**Docker Compose URLs:**

| Service | URL | Credentials | Notes |
|---------|-----|-------------|-------|
| ğŸ¨ React Dashboard | http://localhost:3000 | - | Real-time monitoring UI |
| ğŸ“– **Swagger API Docs** | **http://localhost:5000/api/docs** | - | **Interactive API testing** |
| ğŸ”§ Backend API | http://localhost:5000 | - | REST + WebSocket |
| ğŸ“Š Grafana | http://localhost:3001 | admin/admin | Dashboards & alerts |
| ğŸ“ˆ Prometheus | http://localhost:9090 | - | Metrics query UI |
| ğŸ“ Loki | http://localhost:3100 | - | Log aggregation |
| ğŸ¤– Ollama (ML) | http://localhost:11434 | - | LLM service |

**Kubernetes URLs (Docker Desktop):**

| Service | URL | Credentials | Purpose |
|---------|-----|-------------|---------|
| ğŸ¨ Dashboard | http://localhost | - | Main UI (LoadBalancer) |
| ğŸ“– **Swagger API** | **http://localhost:5000/api/docs** | - | **API documentation** |
| ğŸ”§ Backend API | http://localhost:5000 | - | REST API + WebSocket |
| ğŸ“Š Grafana | http://localhost:3000 | admin/admin | Monitoring dashboards |
| ğŸ“ˆ Prometheus | http://localhost:9090 | - | Metrics database |

> **ğŸ’¡ Tip**: Docker Desktop automatically maps LoadBalancer services to localhost!

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React          â”‚â”€â”€â”€â”€â–¶â”‚  Flask API   â”‚â”€â”€â”€â”€â–¶â”‚  PostgreSQL â”‚
â”‚  Dashboard      â”‚â—€â”€â”€â”€â”€â”‚  + ML Models â”‚â—€â”€â”€â”€â”€â”‚  + ML Data  â”‚
â”‚  (Port 3000)    â”‚ WS  â”‚  (Port 5000) â”‚     â”‚  (Port 5432)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                     â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Remediation  â”‚      â”‚   Ollama    â”‚
             â”‚     Bot      â”‚      â”‚  LLM (3.2)  â”‚
             â”‚  + ML Loop   â”‚      â”‚ Port 11434  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Prometheusâ”‚ â”‚ Grafana  â”‚  â”‚   Loki   â”‚
â”‚  :9090   â”‚ â”‚  :3001   â”‚  â”‚  :3100   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ML Pipeline (6 Phases)
```
Metrics â†’ [Feature Extraction] â†’ [Anomaly Detection] â†’ [Forecasting]
                                          â†“                    â†“
                                  [Failure Prediction] â†’ [LLM Analysis]
                                          â†“
                                  [Continuous Learning]
                                          â†“
                                  [Auto-Retraining]
```

## ğŸ¤– ML/AI Features Deep Dive

### Phase 2: Anomaly Detection
- **Algorithm**: Isolation Forest
- **Features**: 100+ engineered metrics
- **Performance**: 92-95% accuracy, <10ms inference
- **Triggers**: Automatic incident creation for ML-detected anomalies
- **API**: `POST /api/ml/anomaly/train`, `POST /api/ml/anomaly/predict`

### Phase 3: Time Series Forecasting
- **Algorithm**: Facebook Prophet
- **Capabilities**: 1-24 hour ahead predictions
- **Features**: Daily/weekly seasonality detection
- **Use Case**: Proactive breach detection before thresholds hit
- **API**: `POST /api/ml/forecast/train`, `GET /api/ml/forecast/predict`

### Phase 4: LLM Integration
- **Model**: Ollama + Llama 3.2 (3B parameters)
- **Cost**: 100% free (runs locally)
- **Features**: Root cause analysis, natural language reports, remediation suggestions
- **Requirements**: 4GB RAM, ~2GB model download
- **API**: `POST /api/ml/analyze/incident/<id>`, `GET /api/ml/generate-report/<id>`
- **Health Check**: `GET /api/ml/llm/health`

### Phase 5: Failure Prediction
- **Algorithm**: LightGBM (gradient boosting)
- **Prediction Window**: 1-72 hours ahead
- **Risk Levels**: High (70%+), Medium (40-70%), Low (<40%)
- **Proactive Alerts**: Creates "predicted_failure" incidents
- **API**: `POST /api/ml/failure-prediction/train`, `POST /api/ml/failure-prediction/predict`

### Phase 6: Continuous Learning
- **Auto-Retraining**: Every 24 hours or when 100+ new samples collected
- **Performance Monitoring**: Tracks accuracy, precision, MAE
- **Model Versioning**: Full training history in database
- **Triggers**: Time-based, data-volume, performance degradation
- **API**: `GET /api/ml/continuous-learning/status`, `POST /api/ml/continuous-learning/check-retrain`

**Quick ML Setup:**
```bash
# Start with ML services
docker compose -f docker-compose.yml -f docker-compose.ml.yml up -d

# Download LLM model (one-time)
docker exec -it ar_ollama ollama pull llama3.2:3b

# Check ML system status
curl http://localhost:5000/api/ml/continuous-learning/status

# Run ML test scripts
.\scripts\test-phase4.ps1  # LLM integration
.\scripts\test-phase5.ps1  # Failure prediction
.\scripts\test-phase6.ps1  # Continuous learning
```

See [ML Setup Guide](docs/ml-setup.md) for complete documentation.

## Deployment Options

### ğŸ³ Docker Compose (Local/Development)
Single-command setup for local development and testing. All services run in containers with Docker networking.

```bash
docker compose up -d
```

### â˜¸ï¸ Kubernetes (Production/Cloud)
Enterprise-grade deployment with auto-scaling, health checks, and high availability.

- **Raw Manifests**: Full control with `kubectl apply -k k8s/`
- **Helm Charts**: Environment management with `helm install`
- **Kustomize**: Overlay-based configuration

Features:
- Horizontal Pod Autoscaling (2-10 replicas)
- Persistent volumes for databases
- RBAC and service accounts
- LoadBalancers and Ingress
- Multi-environment support (dev/staging/prod)

See [Kubernetes Guide](docs/kubernetes.md) for full deployment instructions.

---

## Development

### Local Setup (Without Docker)

**Backend:**
```powershell
cd app
py -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
python app.py
```

**Bot:**
```powershell
cd bot
py -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
python bot.py
```

**Frontend:**
```powershell
cd frontend
npm install
npm run dev  # http://localhost:5173
```

### Testing

```powershell
# Backend
cd app
pytest tests/ --cov

# Linting
flake8 app bot

# Frontend
cd frontend
npm run build
```

## ğŸ“Š Monitoring & Observability

### Grafana Dashboards
Access at http://localhost:3001 (Docker) or http://localhost:3000 (K8s) with `admin/admin`

**Pre-built Dashboards:**
- **System Overview** - Real-time health, incidents, remediation success rate
- **Application Metrics** - Request rates, latency, error rates by service
- **Infrastructure** - CPU, memory, disk usage across all containers
- **Incident Analysis** - Incident types, frequency, MTTR (Mean Time To Remediation)
- **ML Performance** - Model accuracy, predictions, retraining history

### Prometheus Metrics
Query at http://localhost:9090

**Key Metrics:**
- `http_requests_total` - Total HTTP requests
- `http_request_duration_seconds` - Request latency
- `incidents_total` - Total incidents by type and severity
- `remediations_total` - Successful/failed remediations
- `container_cpu_usage_seconds_total` - Container CPU usage
- `ml_predictions_total` - ML model predictions
- `ml_model_accuracy` - Model performance metrics

### Loki Logs
Access at http://localhost:3100 or via Grafana Explore

**Log Labels:**
- `{container="ar_app"}` - Flask API logs
- `{container="ar_bot"}` - Remediation bot logs
- `{container="ar_frontend"}` - React UI logs

### Alerts
Pre-configured Prometheus alerts in `prometheus/alerts.yml`:
- High error rate (>5% errors)
- Latency spike (p95 > 500ms)
- Resource exhaustion (CPU >90%, Memory >85%)
- Incident rate spike (>10 incidents/5min)

See [Observability Guide](docs/observability.md) for detailed monitoring setup.

## Automation

Includes 7 GitHub Actions workflows for CI/CD:
- **CI** - Testing, linting, building
- **Docker Publish** - GHCR image publishing with semantic versioning
- **Security** - CodeQL, Trivy, dependency scanning
- **Deploy** - AWS ECS and VM deployment
- **Performance** - Locust load testing
- **Release** - Automated GitHub releases
- **Cleanup** - Artifact management

See [docs/github-actions.md](docs/github-actions.md) for details.

## ğŸ“š Documentation

### Essential Guides
- ğŸš€ **[Quick Start Guide](docs/quick-start.md)** - Complete setup for Docker Compose and Kubernetes
- ğŸ¤– **[ML Setup Guide](docs/ml-setup.md)** - All 6 ML phases with examples and API references
- ğŸ“– **[API Reference](docs/API.md)** - REST endpoints, WebSocket events, request/response formats
- â˜¸ï¸ **[Kubernetes Guide](docs/kubernetes.md)** - Production deployment with Helm charts

### Operations
- ğŸ”§ **[Operations Guide](docs/operations.md)** - Configuration, testing, troubleshooting
- ğŸ“Š **[Observability](docs/observability.md)** - Metrics, dashboards, alerts, logging
- ğŸ³ **[Docker Commands](docs/docker.md)** - Container management reference

### Automation
- ğŸš€ **[GitHub Actions](docs/github-actions.md)** - CI/CD workflows and deployment

### API Documentation (Interactive)
- ğŸ“– **Swagger UI**: http://localhost:5000/api/docs (Docker Compose)
- ğŸ“– **Swagger UI**: http://localhost:5000/api/docs (Kubernetes)

**Key API Endpoints:**
- **Core**: `/api/health`, `/api/incidents`, `/api/metrics`, `/api/config`
- **ML Phase 2**: `/api/ml/anomaly/*` - Anomaly detection
- **ML Phase 3**: `/api/ml/forecast/*` - Time series forecasting
- **ML Phase 4**: `/api/ml/analyze/*`, `/api/ml/llm/*` - LLM analysis
- **ML Phase 5**: `/api/ml/failure-prediction/*` - Failure prediction
- **ML Phase 6**: `/api/ml/continuous-learning/*` - Model management

## ğŸ”— Important URLs

### Local Development (Docker Compose)
```
Main Dashboard:        http://localhost:3000
Swagger API Docs:      http://localhost:5000/api/docs
REST API:              http://localhost:5000/api/*
WebSocket:             ws://localhost:5000/ws
Grafana:               http://localhost:3001
Prometheus:            http://localhost:9090
Loki:                  http://localhost:3100
Ollama LLM:            http://localhost:11434
```

### Kubernetes (Docker Desktop)
```
Main Dashboard:        http://localhost
Swagger API Docs:      http://localhost:5000/api/docs
REST API:              http://localhost:5000/api/*
Grafana:               http://localhost:3000
Prometheus:            http://localhost:9090
```

### GitHub Repository
```
Source Code:           https://github.com/mainulhossain123/infra-autofix-agent
CI Pipeline:           https://github.com/mainulhossain123/infra-autofix-agent/actions
Container Registry:    ghcr.io/mainulhossain123/infra-autofix-agent
Issues:                https://github.com/mainulhossain123/infra-autofix-agent/issues
```

## License

MIT Â© 2025
